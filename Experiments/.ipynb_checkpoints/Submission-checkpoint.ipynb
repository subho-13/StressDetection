{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-collection",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.fft as fft\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from numpy import hamming\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-shark",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../WESAD/'\n",
    "SUBJECTS = ['2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '13', '14', '15', '16', '17']\n",
    "CHEST_SIGNALS = ['ECG', 'EMG', 'EDA', 'Resp', 'Temp', 'ACC']\n",
    "CHEST_SAMPLING_RATE = 700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subject_data(subject) :\n",
    "    path = PATH + 'S' + subject + '/S' + subject + '.pkl'\n",
    "    subject = pd.read_pickle(path)\n",
    "    \n",
    "    return subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-income",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_raw(signal, duration, sampling_rate, title) :\n",
    "    n = duration * sampling_rate\n",
    "    l = len(signal)\n",
    "    \n",
    "    s = random.randint(0, l - n)\n",
    "    df = pd.DataFrame(columns=['x', 'y'])\n",
    "    df['x'] = np.arange(n)\n",
    "    df['y'] = signal[s:s+n]\n",
    "    \n",
    "    plt.figure(figsize=(20, 5))\n",
    "    sns.scatterplot(x = 'x', y = 'y', data = df, s = 3).set(title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-faculty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the different chest signals for a random subject\n",
    "\n",
    "SAMPLING_RATE = 700\n",
    "LONG_DURATION = 20\n",
    "SHORT_DURATION = 5\n",
    "\n",
    "subject = random.choice(SUBJECTS)\n",
    "subject_data = read_subject_data(subject)\n",
    "\n",
    "chest_signals = subject_data['signal']['chest']\n",
    "\n",
    "for signal_type in chest_signals :\n",
    "    if signal_type == 'ACC' :\n",
    "        continue\n",
    "    \n",
    "    signal = chest_signals[signal_type]\n",
    "    visualize_raw(signal, LONG_DURATION, SAMPLING_RATE, signal_type + ' - Long Duration')\n",
    "    visualize_raw(signal, SHORT_DURATION, SAMPLING_RATE, signal_type + ' - Short Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-dominant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get additional notes for the Subjects\n",
    "\n",
    "for subject in SUBJECTS :\n",
    "    with open(PATH + 'S' + subject + '/S' + subject + '_readme.txt', 'r') as file :\n",
    "        print(subject, file.readlines()[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "varying-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://imotions.com/blog/eda/\n",
    "# https://www.scitepress.org/Papers/2021/102446/102446.pdf\n",
    "# https://sci-hub.ee/10.1016/j.cmpb.2020.105482\n",
    "WINDOW_LEN = 20\n",
    "OVERLAP = 0.75\n",
    "NUM_FEATURES = 10\n",
    "EPOCH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "spatial-horse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the continuous intervals for each label\n",
    "def find_intervals(labels) :\n",
    "    intervals = []\n",
    "\n",
    "    l = len(labels)\n",
    "    i = 0\n",
    "    label = labels[0]\n",
    "\n",
    "    for j in range(l):\n",
    "        if label != labels[j]:\n",
    "            intervals.append({\n",
    "                'label' : label, \n",
    "                'beg' : i,\n",
    "                'end' : j\n",
    "                })\n",
    "            i = j\n",
    "            label = labels[j]\n",
    "\n",
    "    intervals.append({\n",
    "        'label' : label, \n",
    "        'beg' : i,\n",
    "        'end' : l\n",
    "    })\n",
    "\n",
    "    return intervals    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "great-kelly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fft_features(signal, num_features) :\n",
    "    window = hamming(len(signal))\n",
    "    signal *= window\n",
    "    coeffs = fft.fft(signal)\n",
    "    l = len(coeffs)\n",
    "    freqs = fft.fftfreq(l)\n",
    "    \n",
    "    # Discard the negative elems\n",
    "    l //= 2\n",
    "    amps = np.abs(coeffs[0:l])\n",
    "    freqs = np.abs(freqs[0:l])\n",
    "    \n",
    "    # Sort descending w.r.t amp   \n",
    "    p = amps.argsort()[::-1]\n",
    "    freqs = freqs[p]\n",
    "    amps = amps[p]\n",
    "    \n",
    "    features = [[amps[i], freqs[i]] for i in range(num_features)]   \n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "beautiful-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_fft_features(train_features, test_features) :\n",
    "    feature_mean = np.mean(train_features, axis=0)\n",
    "    print(np.shape(feature_mean))\n",
    "    feature_std = np.std(train_features, axis=0)\n",
    "    train_features -= feature_mean\n",
    "    test_features -= feature_mean\n",
    "    train_features = np.divide(train_features, feature_std, out=np.zeros_like(train_features), where=feature_std!=0)\n",
    "    test_features = np.divide(test_features, feature_std, out=np.zeros_like(test_features), where=feature_std!=0)\n",
    "    \n",
    "    return train_features, test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "handed-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSIENT = 0\n",
    "BASELINE = 1\n",
    "STRESS = 2\n",
    "AMUSEMENT = 3\n",
    "MEDITATION = 4\n",
    "IGNORE = 5\n",
    "\n",
    "def extract_signal_features(signal, intervals, sampling_rate, window_len = WINDOW_LEN, overlap = OVERLAP, num_features = NUM_FEATURES) :\n",
    "    segment_size = sampling_rate * window_len\n",
    "    signal_features = {\n",
    "        BASELINE : [],\n",
    "        STRESS : [],\n",
    "        AMUSEMENT : [],\n",
    "        MEDITATION : []\n",
    "    }\n",
    "    \n",
    "    baseline_av = 0\n",
    "    \n",
    "    for interval in intervals :\n",
    "        label = interval['label']\n",
    "        beg = interval['beg']\n",
    "        end = interval['end']\n",
    "        \n",
    "        signal_of_interest = signal[beg:end]  \n",
    "        \n",
    "        if label >= IGNORE or label == TRANSIENT:\n",
    "            baseline_av = (np.mean(signal_of_interest) + baseline_av)/2\n",
    "            continue      \n",
    "            \n",
    "        if label == BASELINE :\n",
    "            baseline_av = (np.mean(signal_of_interest) + baseline_av)/2\n",
    "        \n",
    "        signal_of_interest -= baseline_av\n",
    "        \n",
    "        l = end - beg        \n",
    "        while l >= segment_size:\n",
    "            segment = signal_of_interest[int(l - segment_size) : l]\n",
    "            l -= int((1 - overlap) * segment_size)\n",
    "            \n",
    "            segment_features = extract_fft_features(segment, num_features)            \n",
    "            signal_features[label].append(segment_features)\n",
    "        \n",
    "    return signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "metallic-tension",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Joint test-train  Datasets combining all signals\n",
    "\n",
    "def get_chest_signal_dataset(subjects, signal_types = CHEST_SIGNALS) :\n",
    "    combined_subject_features = {}\n",
    "    \n",
    "    for subject in subjects :\n",
    "        subject_data = read_subject_data(subject)\n",
    "        intervals = find_intervals(subject_data['label'])\n",
    "        \n",
    "        subject_features = {\n",
    "            BASELINE : [],\n",
    "            STRESS : [],\n",
    "            AMUSEMENT : [],\n",
    "            MEDITATION : []\n",
    "        }\n",
    "        \n",
    "        for signal_type in signal_types :\n",
    "            if signal_type == 'ACC' :\n",
    "                continue\n",
    "                \n",
    "            signal = np.array(subject_data['signal']['chest'][signal_type]).flatten()\n",
    "            signal_features = extract_signal_features(signal, intervals, CHEST_SAMPLING_RATE)\n",
    "            \n",
    "            for label, features in signal_features.items() :\n",
    "                subject_features[label].append(features)\n",
    "                \n",
    "        for label in subject_features :\n",
    "            subject_features[label] = np.stack(subject_features[label], axis = -1)\n",
    "            \n",
    "        aggregate_subject_features = []\n",
    "        aggregate_subject_labels = []\n",
    "            \n",
    "        for label, features in subject_features.items() :\n",
    "            for feature in features :\n",
    "                aggregate_subject_features.append(feature)\n",
    "                aggregate_subject_labels.append(label)\n",
    "                \n",
    "        feature_mean = np.mean(aggregate_subject_features, axis=0)\n",
    "        feature_std = np.std(aggregate_subject_features, axis=0)\n",
    "        \n",
    "        aggregate_subject_features = np.array(aggregate_subject_features) - feature_mean\n",
    "        aggregate_subject_features = np.divide(aggregate_subject_features, feature_std, out=np.zeros_like(aggregate_subject_features), where=feature_std!=0)\n",
    "                \n",
    "        combined_subject_features[subject] = {\n",
    "            'features' : aggregate_subject_features,\n",
    "            'labels' : aggregate_subject_labels\n",
    "        }\n",
    "\n",
    "    return combined_subject_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "killing-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(combined_subject_features, subjects) :\n",
    "    features_dataset = []\n",
    "    labels_dataset = []\n",
    "    \n",
    "    for subject in subjects :\n",
    "        features_dataset += list(combined_subject_features[subject]['features'])\n",
    "        labels_dataset += combined_subject_features[subject]['labels']\n",
    "        \n",
    "    return np.array(features_dataset), np.array(labels_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "threaded-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels) :\n",
    "    encoder = {\n",
    "        1 : [1, 0, 0, 0],\n",
    "        2 : [0, 1, 0, 0],\n",
    "        3 : [0, 0, 1, 0],\n",
    "        4 : [0, 0, 0, 1]\n",
    "    }\n",
    "    \n",
    "    return np.array([np.array(encoder[l]) for l in labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "damaged-aside",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(model, test_features, test_labels) :\n",
    "    predicted_labels = model.predict(test_features)\n",
    "\n",
    "    confusion_matrix = np.zeros((4, 4))\n",
    "\n",
    "    for test_label, predicted_label in zip(test_labels, predicted_labels) :\n",
    "        i = np.argmax(test_label)\n",
    "        j = np.argmax(predicted_label)\n",
    "\n",
    "        confusion_matrix[i][j] += 1\n",
    "        \n",
    "    confusion_matrix /= np.sum(confusion_matrix, axis=1).reshape(4, 1)\n",
    "    \n",
    "    return confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "alive-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_subject_features = get_chest_signal_dataset(SUBJECTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "rental-escape",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_weights = '../Models/init_weights.hdf5'\n",
    "best_model = '../Models/best_model.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "communist-rider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "depthwise_conv2d_28 (Depthwi (None, 9, 2, 10)          30        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_29 (Depthwi (None, 8, 2, 10)          30        \n",
      "_________________________________________________________________\n",
      "spatial_dropout2d_7 (Spatial (None, 8, 2, 10)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 8, 2, 6)           66        \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_30 (Depthwi (None, 6, 2, 6)           24        \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 4, 2, 6)           114       \n",
      "_________________________________________________________________\n",
      "depthwise_conv2d_31 (Depthwi (None, 2, 2, 6)           24        \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 2, 1, 8)           104       \n",
      "_________________________________________________________________\n",
      "conv2d_31 (Conv2D)           (None, 1, 1, 8)           136       \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 36        \n",
      "=================================================================\n",
      "Total params: 668\n",
      "Trainable params: 652\n",
      "Non-trainable params: 16\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=(10, 2, 5)),\n",
    "    keras.layers.DepthwiseConv2D(kernel_size=(2,1), activation='swish', depth_multiplier=2),\n",
    "    keras.layers.DepthwiseConv2D(kernel_size=(2,1), activation='swish'),\n",
    "    keras.layers.SpatialDropout2D(0.5),\n",
    "    keras.layers.Conv2D(6, kernel_size=(1, 1), activation='swish'),\n",
    "    keras.layers.DepthwiseConv2D(kernel_size=(3,1), activation='swish'),\n",
    "    keras.layers.Conv2D(6, kernel_size=(3,1), activation='swish'),\n",
    "    keras.layers.DepthwiseConv2D(kernel_size=(3,1), activation='swish'),\n",
    "    keras.layers.Conv2D(8, kernel_size=(1,2), activation='swish'),\n",
    "    keras.layers.Conv2D(8, kernel_size=(2,1), activation='swish'),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(units=8,  activation='swish'),\n",
    "    keras.layers.Dense(units=4, activation='softmax')\n",
    "])\n",
    "model.save_weights(init_weights)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "wound-darwin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=tf.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "native-sample",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1024\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.9356 - accuracy: 0.5877 - val_loss: 0.9280 - val_accuracy: 0.5544\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.92801, saving model to ../Models/best_model.hdf5\n",
      "Epoch 2/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.7053 - accuracy: 0.7032 - val_loss: 0.6458 - val_accuracy: 0.7291\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.92801 to 0.64584, saving model to ../Models/best_model.hdf5\n",
      "Epoch 3/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.6379 - accuracy: 0.7340 - val_loss: 0.6385 - val_accuracy: 0.6970\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64584 to 0.63851, saving model to ../Models/best_model.hdf5\n",
      "Epoch 4/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.6249 - accuracy: 0.7378 - val_loss: 0.6192 - val_accuracy: 0.7112\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.63851 to 0.61916, saving model to ../Models/best_model.hdf5\n",
      "Epoch 5/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.6097 - accuracy: 0.7445 - val_loss: 0.4943 - val_accuracy: 0.7576\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.61916 to 0.49433, saving model to ../Models/best_model.hdf5\n",
      "Epoch 6/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.5836 - accuracy: 0.7547 - val_loss: 0.4499 - val_accuracy: 0.8021\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.49433 to 0.44991, saving model to ../Models/best_model.hdf5\n",
      "Epoch 7/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.5726 - accuracy: 0.7627 - val_loss: 0.3523 - val_accuracy: 0.8627\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.44991 to 0.35228, saving model to ../Models/best_model.hdf5\n",
      "Epoch 8/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.5525 - accuracy: 0.7741 - val_loss: 0.3589 - val_accuracy: 0.8467\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.35228\n",
      "Epoch 9/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.5381 - accuracy: 0.7769 - val_loss: 0.3210 - val_accuracy: 0.8681\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.35228 to 0.32103, saving model to ../Models/best_model.hdf5\n",
      "Epoch 10/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.5097 - accuracy: 0.7943 - val_loss: 0.3353 - val_accuracy: 0.8645\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.32103\n",
      "Epoch 11/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4897 - accuracy: 0.8009 - val_loss: 0.3893 - val_accuracy: 0.8663\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.32103\n",
      "Epoch 12/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4930 - accuracy: 0.8042 - val_loss: 0.5335 - val_accuracy: 0.7522\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.32103\n",
      "Epoch 13/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4751 - accuracy: 0.8103 - val_loss: 0.5031 - val_accuracy: 0.6631\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.32103\n",
      "Epoch 14/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4750 - accuracy: 0.8054 - val_loss: 0.3144 - val_accuracy: 0.8788\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.32103 to 0.31439, saving model to ../Models/best_model.hdf5\n",
      "Epoch 15/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4669 - accuracy: 0.8109 - val_loss: 0.4002 - val_accuracy: 0.8699\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.31439\n",
      "Epoch 16/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4589 - accuracy: 0.8136 - val_loss: 0.5874 - val_accuracy: 0.6720\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.31439\n",
      "Epoch 17/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4608 - accuracy: 0.8142 - val_loss: 0.4007 - val_accuracy: 0.8770\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.31439\n",
      "Epoch 18/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4513 - accuracy: 0.8180 - val_loss: 0.6431 - val_accuracy: 0.6774\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.31439\n",
      "Epoch 19/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4439 - accuracy: 0.8190 - val_loss: 0.3321 - val_accuracy: 0.8788\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.31439\n",
      "Epoch 20/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4498 - accuracy: 0.8192 - val_loss: 0.4027 - val_accuracy: 0.8699\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.31439\n",
      "Epoch 21/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4452 - accuracy: 0.8213 - val_loss: 0.3959 - val_accuracy: 0.7932\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.31439\n",
      "Epoch 22/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4441 - accuracy: 0.8268 - val_loss: 0.5143 - val_accuracy: 0.7879\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.31439\n",
      "Epoch 23/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4386 - accuracy: 0.8232 - val_loss: 0.6990 - val_accuracy: 0.6275\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.31439\n",
      "Epoch 24/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4444 - accuracy: 0.8212 - val_loss: 0.6763 - val_accuracy: 0.6328\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.31439\n",
      "Epoch 25/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4277 - accuracy: 0.8273 - val_loss: 0.4912 - val_accuracy: 0.7861\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.31439\n",
      "Epoch 26/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4303 - accuracy: 0.8256 - val_loss: 0.6455 - val_accuracy: 0.7112\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.31439\n",
      "Epoch 27/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4284 - accuracy: 0.8283 - val_loss: 0.6058 - val_accuracy: 0.7094\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.31439\n",
      "Epoch 28/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4356 - accuracy: 0.8229 - val_loss: 1.2125 - val_accuracy: 0.5437\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.31439\n",
      "Epoch 29/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4369 - accuracy: 0.8259 - val_loss: 0.4447 - val_accuracy: 0.8592\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.31439\n",
      "Epoch 30/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4320 - accuracy: 0.8263 - val_loss: 0.7163 - val_accuracy: 0.6399\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.31439\n",
      "Epoch 31/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4291 - accuracy: 0.8258 - val_loss: 0.6056 - val_accuracy: 0.7166\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.31439\n",
      "Epoch 32/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4255 - accuracy: 0.8301 - val_loss: 0.5502 - val_accuracy: 0.7932\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.31439\n",
      "Epoch 33/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4296 - accuracy: 0.8316 - val_loss: 1.3895 - val_accuracy: 0.5027\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.31439\n",
      "Epoch 34/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4296 - accuracy: 0.8265 - val_loss: 0.9133 - val_accuracy: 0.5918\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.31439\n",
      "Epoch 35/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4428 - accuracy: 0.8220 - val_loss: 0.6125 - val_accuracy: 0.6988\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.31439\n",
      "Epoch 36/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4151 - accuracy: 0.8350 - val_loss: 0.4708 - val_accuracy: 0.8574\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.31439\n",
      "Epoch 37/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4095 - accuracy: 0.8349 - val_loss: 0.8911 - val_accuracy: 0.5758\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.31439\n",
      "Epoch 38/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4167 - accuracy: 0.8311 - val_loss: 1.0214 - val_accuracy: 0.5401\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.31439\n",
      "Epoch 39/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4325 - accuracy: 0.8298 - val_loss: 0.5133 - val_accuracy: 0.7986\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.31439\n",
      "Epoch 40/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4176 - accuracy: 0.8319 - val_loss: 0.4883 - val_accuracy: 0.7647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00040: val_loss did not improve from 0.31439\n",
      "Epoch 41/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4103 - accuracy: 0.8345 - val_loss: 0.4170 - val_accuracy: 0.8271\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.31439\n",
      "Epoch 42/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4241 - accuracy: 0.8284 - val_loss: 0.5629 - val_accuracy: 0.7451\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 0.31439\n",
      "Epoch 43/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4190 - accuracy: 0.8362 - val_loss: 1.5151 - val_accuracy: 0.5294\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.31439\n",
      "Epoch 44/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4179 - accuracy: 0.8338 - val_loss: 0.8698 - val_accuracy: 0.6007\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.31439\n",
      "Epoch 45/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4077 - accuracy: 0.8355 - val_loss: 1.0790 - val_accuracy: 0.5775\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.31439\n",
      "Epoch 46/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4112 - accuracy: 0.8340 - val_loss: 0.6845 - val_accuracy: 0.6988\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.31439\n",
      "Epoch 47/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4084 - accuracy: 0.8374 - val_loss: 0.8295 - val_accuracy: 0.5936\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.31439\n",
      "Epoch 48/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4086 - accuracy: 0.8336 - val_loss: 1.0561 - val_accuracy: 0.5775\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.31439\n",
      "Epoch 49/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4061 - accuracy: 0.8373 - val_loss: 0.5208 - val_accuracy: 0.7594\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.31439\n",
      "Epoch 50/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4109 - accuracy: 0.8365 - val_loss: 0.3972 - val_accuracy: 0.8574\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.31439\n",
      "Epoch 51/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4053 - accuracy: 0.8401 - val_loss: 0.4650 - val_accuracy: 0.8289\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.31439\n",
      "Epoch 52/1024\n",
      "512/512 [==============================] - 1s 1ms/step - loss: 0.4100 - accuracy: 0.8362 - val_loss: 1.1324 - val_accuracy: 0.5419\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 0.31439\n",
      "Epoch 53/1024\n",
      "512/512 [==============================] - 1s 2ms/step - loss: 0.4019 - accuracy: 0.8400 - val_loss: 0.4420 - val_accuracy: 0.8057\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.31439\n",
      "Epoch 54/1024\n",
      "175/512 [=========>....................] - ETA: 0s - loss: 0.3994 - accuracy: 0.8343"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-2342fd4b09ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     model.fit(train_features, train_labels, epochs=EPOCH, batch_size = 16, verbose=1, shuffle=True,\n\u001b[0m\u001b[1;32m     23\u001b[0m                   validation_data=(test_features,  test_labels), callbacks=[model_checkpoint_callback])\n\u001b[1;32m     24\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1181\u001b[0m                 _r=1):\n\u001b[1;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1183\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1184\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[1;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3023\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1959\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 591\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "subject_conf_matrix = {}\n",
    "\n",
    "for i in range(len(SUBJECTS)) :\n",
    "    test_subject = SUBJECTS[i]\n",
    "    train_subjects = SUBJECTS[:i] + SUBJECTS[i+1:]\n",
    "    \n",
    "    train_features, train_labels = generate_dataset(combined_subject_features, train_subjects)\n",
    "    test_features, test_labels = generate_dataset(combined_subject_features, [test_subject])\n",
    "    \n",
    "    train_labels = encode_labels(train_labels)\n",
    "    test_labels = encode_labels(test_labels)\n",
    "    \n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=best_model,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    model.load_weights(init_weights)\n",
    "    model.fit(train_features, train_labels, epochs=EPOCH, batch_size = 16, verbose=1, shuffle=True,\n",
    "                  validation_data=(test_features,  test_labels), callbacks=[model_checkpoint_callback])\n",
    "    model.load_weights(best_model)\n",
    "    \n",
    "    subject_conf_matrix[test_subject] = get_confusion_matrix(model, test_features, test_labels)  \n",
    "    print(test_subject, subject_conf_matrix[test_subject])\n",
    "\n",
    "    with open(\"confusion_matrix.pkl\", 'wb') as file :\n",
    "        pickle.dump(subject_conf_matrix, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-register",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
